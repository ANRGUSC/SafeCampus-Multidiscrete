# sweep.yaml
method: grid
metric:
  name: avg_episode_reward
  goal: maximize
parameters:
  alpha:
    values: [0.5] # example alpha values
#  sigma_init:
#    values: [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.1] # example sigma_init values
#  hidden_units:
#    values: [16, 28, 32, 64, 72, 128, 256, 512]
#  batch_size:
#    values: [16, 28, 32, 64, 72,  128, 256, 512]
#  softmax_temperature:
#    values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]
#  e_decay_function:
#    values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
  learning_rate:
    values: [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.1]
#  discount_factor:
#    values: [0.1]
#  exploration_rate:
#    values: [1.0]
#  exploration_decay_rate:
#    values: [0.00001]
#  min_exploration_rate:
#    values: [0.000001]
#  learning_rate_decay:
#    values: [0.99995, 0.9995, 0.999, 0.995, 0.99, 0.95, 0.9]
#  min_learning_rate:
#    values: [0.00000000000001, 0.0000000001]

## sweep_random.yaml
#method: random
#metric:
#  name: average_return
#  goal: maximize
#parameters:
#  alpha:
#    distribution: uniform
#    min: 0.1
#    max: 0.99
#  learning_rate:
#    distribution: uniform
#    min: 0.0001
#    max: 0.1
#  discount_factor:
#    distribution: uniform
#    min: 0.5
#    max: 0.99
#  exploration_rate:
#    distribution: uniform
#    min: 0.1
#    max: 1.0
#  exploration_decay_rate:
#    distribution: uniform
#    min: 0.8
#    max: 0.998
#  min_exploration_rate:
#    distribution: uniform
#    min: 0.00000001
#    max: 0.00001

