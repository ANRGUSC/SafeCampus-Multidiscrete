{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SafeCampus","text":"<p>A tool for evaluating safety of control policies for an indoor stochastic epidemic model.</p>"},{"location":"#evaluation-pipeline","title":"Evaluation Pipeline","text":""},{"location":"agent/","title":"Agent Design Tutorial: Step-by-Step Process","text":"<p>This tutorial provides a logical, step-by-step guide on how to design and implement a new agent for your reinforcement learning environment. Follow these steps to create your custom agent efficiently.</p>"},{"location":"agent/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Choose Agent Type and Naming</li> <li>Create Configuration File</li> <li>Design Network Architecture</li> <li>Implement Agent Class</li> <li>Develop Action Selection Process</li> <li>Implement Training Loop</li> <li>Add Model Saving and Loading</li> <li>Create Evaluation Method</li> <li>Develop Visualization Tools</li> </ol>"},{"location":"agent/#1-choose-agent-type-and-naming","title":"1. Choose Agent Type and Naming","text":"<p>Start by deciding on the type of agent you want to create (e.g., DQN, PPO, A2C) and choose appropriate names:</p> <ul> <li>Root identifier: <code>&lt;agent&gt;_custom</code> (e.g., <code>dqn_custom</code>, <code>ppo_custom</code>)</li> <li>Class name: <code>&lt;Agent&gt;CustomAgent</code> (e.g., <code>DQNCustomAgent</code>, <code>PPOCustomAgent</code>)</li> </ul> <p>Replace <code>&lt;agent&gt;</code> and <code>&lt;Agent&gt;</code> with your chosen agent type.</p>"},{"location":"agent/#2-create-configuration-file","title":"2. Create Configuration File","text":"<p>Create a YAML configuration file (e.g., <code>dqn_custom_config.yaml</code>) to store all hyperparameters and settings:</p> <ul> <li>Network architecture details</li> <li>Learning parameters</li> <li>Training settings</li> <li>Environment-specific parameters</li> </ul> <p>This file will make it easier to experiment with different settings and reproduce results.</p>"},{"location":"agent/#3-design-network-architecture","title":"3. Design Network Architecture","text":"<p>Outline your neural network architecture:</p> <ul> <li>Determine input and output dimensions based on your environment</li> <li>Decide on the number and size of hidden layers</li> <li>Choose appropriate activation functions</li> </ul> <p>Consider the complexity of your environment when designing the network.</p>"},{"location":"agent/#4-implement-agent-class","title":"4. Implement Agent Class","text":"<p>Create your agent class (e.g., <code>DQNCustomAgent</code>):</p> <ul> <li>Initialize the neural network(s)</li> <li>Set up the optimizer</li> <li>Initialize any necessary memory buffers or data structures</li> <li>Load hyperparameters from the configuration file</li> </ul> <p>This class will serve as the core of your agent implementation.</p>"},{"location":"agent/#5-develop-action-selection-process","title":"5. Develop Action Selection Process","text":"<p>Implement the action selection method:</p> <ul> <li>Convert environment state to network input</li> <li>Pass the state through your network</li> <li>Implement an exploration strategy (e.g., epsilon-greedy)</li> </ul> <p>Balance exploration and exploitation, especially early in training.</p>"},{"location":"agent/#6-implement-training-loop","title":"6. Implement Training Loop","text":"<p>Create the main training loop:</p> <ul> <li>Initialize the environment</li> <li>Iterate through episodes</li> <li>Select actions and interact with the environment</li> <li>Store experiences (if using experience replay)</li> <li>Perform learning updates</li> </ul> <p>This is where your agent will learn from its interactions with the environment.</p>"},{"location":"agent/#7-add-model-saving-and-loading","title":"7. Add Model Saving and Loading","text":"<p>Implement functions to:</p> <ul> <li>Save the trained model and its state</li> <li>Load a previously saved model</li> </ul> <p>This allows for interrupting and resuming training, and using trained models for evaluation.</p>"},{"location":"agent/#8-create-evaluation-method","title":"8. Create Evaluation Method","text":"<p>Develop a method to evaluate your agent's performance:</p> <ul> <li>Load a trained model</li> <li>Run the agent through test episodes without training</li> <li>Collect and analyze performance metrics</li> </ul> <p>This helps in assessing how well your agent has learned.</p>"},{"location":"agent/#9-develop-visualization-tools","title":"9. Develop Visualization Tools","text":"<p>Create tools to visualize your agent's performance and behavior:</p> <ul> <li>Plot learning curves (e.g., rewards over time, loss values)</li> <li>Visualize the agent's behavior in the environment</li> <li>Create policy heatmaps or other relevant visualizations</li> </ul> <p>Good visualizations can provide insights into your agent's learning process and aid in debugging.</p> <p>By following this step-by-step process, you can design and implement your custom agent in a structured and organized manner. Remember to iterate on your design, test thoroughly, and adapt each step to the specific needs of your agent and environment.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>This section explains how to configure the project using the provided YAML files.</p>"},{"location":"configuration/#shared-configuration","title":"Shared Configuration","text":"<p>The shared configuration file is located at <code>config/config_shared.yaml</code> and includes:</p> <ul> <li><code>wandb</code> settings</li> <li>Environment settings</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed: - Python 3.8 or higher - pip (Python package installer) - Conda (for environment management)</p>"},{"location":"installation/#installation-steps","title":"Installation Steps","text":""},{"location":"installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>First, clone the project repository from GitHub:</p> <pre><code>git clone https://github.com/your-github-username/your-repo-name.git\ncd your-repo-name\n</code></pre>"},{"location":"installation/#2-set-up-conda-environment","title":"2. Set Up Conda Environment","text":"<p>Create and activate a new conda environment:</p> <pre><code>conda create -n campus_gym\nconda activate campus_gym\n</code></pre>"},{"location":"installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<p>Install the required packages using pip:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"installation/#4-configure-weights-biases-wandb","title":"4. Configure Weights &amp; Biases (wandb)","text":"<p>If you haven't already, sign up for a wandb account at https://wandb.ai. Then, log in via the command line:</p> <pre><code>wandb login\n</code></pre>"},{"location":"installation/#5-set-up-configuration-files","title":"5. Set Up Configuration Files","text":"<p>Ensure all configuration files are in place and update the wandb settings with your username and project name. The configuration files are located in the <code>config</code> directory and include the following:</p> <ul> <li><code>config/config_shared.yaml</code>: Shared configuration settings</li> <li><code>config/config_{agent_type}.yaml</code>: Agent-specific configurations</li> <li><code>config/optuna_config.yaml</code>: Optuna hyperparameter optimization settings (if using)</li> </ul> <p>For detailed usage instructions, please refer to the Usage Guide.</p>"},{"location":"results/","title":"Results","text":"<p>This section provides detailed information on how to view, analyze, and interpret the results generated by the project. The results include training metrics, evaluation metrics, visualizations, and safety analysis.</p>"},{"location":"results/#1-viewing-results","title":"1. Viewing Results","text":""},{"location":"results/#11-weights-biases-integration","title":"1.1. Weights &amp; Biases Integration","text":"<p>The project uses Weights &amp; Biases (W&amp;B) to log and visualize training and evaluation metrics. You can view the results by logging into your W&amp;B account and navigating to the project dashboard.</p>"},{"location":"results/#111-metrics-logged","title":"1.1.1. Metrics Logged","text":"<ul> <li>Cumulative Reward: The total reward accumulated over the course of an episode.</li> <li>Average Reward: The mean reward per episode.</li> <li>Discounted Reward: The reward calculated with a discount factor applied to future rewards.</li> <li>Sample Efficiency: The number of unique states visited during an episode.</li> <li>Policy Entropy: The entropy of the policy, representing the uncertainty in action selection.</li> <li>Space Complexity: The total number of parameters in the model.</li> </ul>"},{"location":"results/#12-local-csv-files","title":"1.2. Local CSV Files","text":"<p>For each training and evaluation run, metrics are also saved locally as CSV files in the <code>results</code> directory. The following CSV files are generated:</p> <ul> <li><code>training_metrics_&lt;run_name&gt;.csv</code>: Contains training metrics for each episode.</li> <li><code>evaluation_metrics_&lt;run_name&gt;.csv</code>: Contains evaluation metrics for each step within an episode.</li> <li><code>mean_allowed_infected.csv</code>: Summarizes the mean allowed and infected values across all episodes.</li> </ul> <p>These files can be found in the results subdirectory specific to each run, typically located at <code>&lt;results_directory&gt;/&lt;agent_type&gt;/&lt;run_name&gt;/&lt;timestamp&gt;/</code>.</p>"},{"location":"results/#2-visualizations","title":"2. Visualizations","text":"<p>All visualizations are saved in the results subdirectory for each run. The specific path is <code>&lt;results_directory&gt;/&lt;agent_type&gt;/&lt;run_name&gt;/&lt;timestamp&gt;/</code>.</p>"},{"location":"results/#21-tolerance-interval-curve","title":"2.1. Tolerance Interval Curve","text":"<p>The Tolerance Interval Curve shows the range of expected returns within a specified confidence level (<code>alpha</code>) and proportion (<code>beta</code>). This curve helps visualize the performance consistency of the model across different runs.</p> <ul> <li><code>tolerance_interval_mean.png</code>: The mean performance with the tolerance interval.</li> <li><code>tolerance_interval_median.png</code>: The median performance with the tolerance interval.</li> </ul>"},{"location":"results/#22-confidence-interval-curve","title":"2.2. Confidence Interval Curve","text":"<p>The Confidence Interval Curve shows the mean performance of the model with a confidence interval, typically at 95%. This visualization helps assess the reliability of the model's performance.</p> <ul> <li><code>confidence_interval.png</code>: The confidence interval for mean performance.</li> </ul>"},{"location":"results/#23-safety-set-identification","title":"2.3. Safety Set Identification","text":"<p>The Safety Set Identification plot shows the states where the model's policy maintains safety constraints, such as keeping the number of infections below a threshold.</p> <ul> <li><code>safety_set_plot_episode_&lt;run_name&gt;.png</code>: A plot showing the safety set for a specific episode.</li> </ul>"},{"location":"results/#24-lyapunov-function-behavior","title":"2.4. Lyapunov Function Behavior","text":"<p>The Lyapunov Function Behavior plots demonstrate the stability of the system, particularly in the Disease-Free Equilibrium (DFE) and Endemic Equilibrium (EE) regions.</p> <ul> <li><code>lyapunov_loss_plot.png</code>: The loss function during the training of the Lyapunov function.</li> <li><code>lyapunov_function_with_context_&lt;method_label&gt;.png</code>: The Lyapunov function behavior with context for DFE and EE regions.</li> </ul>"},{"location":"results/#25-transition-matrix","title":"2.5. Transition Matrix","text":"<p>The Transition Matrix visualizes the probability of transitioning from one state to another based on the community risk values. This matrix is useful for understanding the model's behavior under varying conditions.</p> <ul> <li><code>transition_matrix_&lt;run_name&gt;.png</code>: The transition probability matrix based on community risk.</li> </ul>"},{"location":"results/#26-q-table-visualization-q-learning-only","title":"2.6. Q-Table Visualization (Q-learning only)","text":"<p>For Q-learning, a heatmap of the Q-table is generated to visualize the learned state-action values.</p> <ul> <li><code>q_table_heatmap.png</code>: Heatmap visualization of the Q-table.</li> </ul>"},{"location":"results/#27-states-visited-visualization","title":"2.7. States Visited Visualization","text":"<p>Heatmaps showing the frequency of visited states during training are generated for both DQN and Q-learning.</p> <ul> <li>Multiple PNG files with names containing <code>states_visited</code> are generated in the results subdirectory.</li> </ul>"},{"location":"results/#28-evaluation-results-plot","title":"2.8. Evaluation Results Plot","text":"<p>For Q-learning, an evaluation plot is generated showing the allowed students, infected individuals, and community risk over time.</p> <ul> <li><code>evaluation_plot_&lt;run_name&gt;.png</code>: Plot of evaluation results.</li> </ul>"},{"location":"results/#3-safety-analysis","title":"3. Safety Analysis","text":""},{"location":"results/#31-safety-set-conditions","title":"3.1. Safety Set Conditions","text":"<p>The safety set conditions are logged in the <code>safety_conditions_&lt;run_name&gt;.csv</code> file. This file contains the following columns:</p> <ul> <li>Episode: The episode number.</li> <li>Infections &gt; Threshold (%): The percentage of time the number of infections exceeded the threshold.</li> <li>Safety Condition Met (Infection): Indicates whether the infection safety condition was met.</li> <li>Allowed Students \u2265 Threshold (%): The percentage of time the allowed students met or exceeded the threshold.</li> <li>Safety Condition Met (Attendance): Indicates whether the attendance safety condition was met.</li> </ul>"},{"location":"results/#32-control-barrier-functions-cbfs","title":"3.2. Control Barrier Functions (CBFs)","text":"<p>The CBFs are used to ensure that the system maintains safety constraints over time. The verification of forward invariance is logged in the <code>cbf_verification.txt</code> file.</p> <ul> <li>CBF for Infections: Ensures that the number of infected individuals does not exceed the threshold.</li> <li>CBF for Attendance: Ensures that the number of allowed students meets the required threshold.</li> </ul>"},{"location":"results/#4-final-results-summary","title":"4. Final Results Summary","text":"<p>At the end of the evaluation process, a summary of the final results is saved in the <code>final_results.txt</code> file. This summary includes:</p> <ul> <li>Cumulative Reward across all episodes</li> <li>Average Reward per episode</li> <li>Safety condition percentages for infections and attendance</li> <li>Forward Invariance verification</li> <li>Stability assessment in the DFE and EE regions</li> </ul> <p>This file provides a quick overview of the model's performance and safety compliance across the entire evaluation period.</p> <p>Use the above results and visualizations to assess and refine your model. The combination of metrics, visualizations, and safety analysis will help you understand the model's strengths and weaknesses and guide further development. Remember to check the specific results subdirectory for each run to find all the generated files and visualizations.</p>"},{"location":"usage/","title":"Usage","text":"<p>Prepare Community Risk CSV (if applicable) If you're planning to use community risk data from a CSV file, ensure it's placed in the appropriate directory and update the <code>csv_path</code> argument when running the script.</p>"},{"location":"usage/#running-the-project","title":"Running the Project","text":"<p>You can run the project in different modes:</p> <ol> <li>Training:</li> </ol> <pre><code>python main.py train --agent_type q_learning --alpha 0.8 --algorithm q_learning\n</code></pre> <ol> <li>Evaluation:</li> </ol> <pre><code>python main.py eval --agent_type q_learning --alpha 0.8 --run_name your_run_name --csv_path path/to/your/csv --algorithm q_learning\n</code></pre> <ol> <li>Combined Training and Evaluation:</li> </ol> <pre><code>python main.py train_and_eval --agent_type q_learning --alpha 0.8 --csv_path path/to/your/csv --algorithm q_learning\n</code></pre> <ol> <li>Hyperparameter Sweep:</li> </ol> <pre><code>python main.py sweep --agent_type q_learning\n</code></pre> <ol> <li>Multiple Runs:</li> </ol> <pre><code>python main.py multi --agent_type q_learning --alpha_t 0.05 --beta_t 0.9 --num_runs 10\n</code></pre> <ol> <li>Optuna Optimization:</li> </ol> <pre><code>python main.py optuna --agent_type q_learning\n</code></pre> <p>Replace <code>q_learning</code> with <code>dqn</code> if you want to use the DQN algorithm instead.</p>"},{"location":"usage/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation or running the project, please check the following:</p> <ol> <li>Ensure all prerequisites are correctly installed.</li> <li>Verify that you're in the correct conda environment (<code>conda activate campus_gym</code>).</li> <li>Check that all configuration files are properly set up.</li> <li>Make sure you have the necessary permissions to read/write in the project directory.</li> </ol> <p>If problems persist, please open an issue on the GitHub repository with details about the error and your environment.</p>"}]}